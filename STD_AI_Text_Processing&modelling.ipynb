{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STD_AI_Text_Processing&modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rTbaoFhwmiTh_NiDtUgmqlBaih5RsRDt",
      "authorship_tag": "ABX9TyPEIJPjP9qLjuy+ts9VDBby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshurastogi29/how-effective-are-std-drugs/blob/master/STD_AI_Text_Processing%26modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfg9b3-L7r4s",
        "colab_type": "code",
        "outputId": "48641a23-b8dd-482a-d9eb-7aaf7eb82ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "%tensorflow_version 1.14"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yINF50SEf_5G",
        "colab_type": "code",
        "outputId": "b2b50308-5ca6-45df-8edd-53fecc510fea",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d takuok/glove840b300dtxt\n",
        "!kaggle datasets download -d yekenot/fasttext-crawl-300d-2m\n",
        "!unzip glove840b300dtxt.zip\n",
        "!unzip fasttext-crawl-300d-2m.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0d9a548d-ebb4-4e17-83a1-b2ace4e63522\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0d9a548d-ebb4-4e17-83a1-b2ace4e63522\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 65 Apr 10 11:07 kaggle.json\n",
            "Downloading glove840b300dtxt.zip to /content\n",
            " 99% 2.06G/2.08G [00:25<00:00, 73.2MB/s]\n",
            "100% 2.08G/2.08G [00:25<00:00, 87.6MB/s]\n",
            "Downloading fasttext-crawl-300d-2m.zip to /content\n",
            " 99% 1.43G/1.44G [00:36<00:00, 40.9MB/s]\n",
            "100% 1.44G/1.44G [00:36<00:00, 42.7MB/s]\n",
            "Archive:  glove840b300dtxt.zip\n",
            "  inflating: glove.840B.300d.txt     \n",
            "Archive:  fasttext-crawl-300d-2m.zip\n",
            "  inflating: crawl-300d-2M.vec       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt68pVNuiOHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "import random\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEWEFAsPFKR",
        "colab_type": "code",
        "outputId": "095c2aa9-b593-4a67-d10b-7da316a4fb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "df = df.drop(columns=['drug_approved_by_UIC','patient_id'])\n",
        "df1 = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name_of_drug</th>\n",
              "      <th>use_case_for_drug</th>\n",
              "      <th>review_by_patient</th>\n",
              "      <th>effectiveness_rating</th>\n",
              "      <th>number_of_times_prescribed</th>\n",
              "      <th>base_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Valsartan</td>\n",
              "      <td>Left Ventricular Dysfunction</td>\n",
              "      <td>\"It has no side effect, I take it in combinati...</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>8.022969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guanfacine</td>\n",
              "      <td>ADHD</td>\n",
              "      <td>\"My son is halfway through his fourth week of ...</td>\n",
              "      <td>8</td>\n",
              "      <td>192</td>\n",
              "      <td>7.858458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lybrel</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>6.341969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Buprenorphine / naloxone</td>\n",
              "      <td>Opiate Dependence</td>\n",
              "      <td>\"Suboxone has completely turned my life around...</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "      <td>6.590176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cialis</td>\n",
              "      <td>Benign Prostatic Hyperplasia</td>\n",
              "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>6.144782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               name_of_drug  ... base_score\n",
              "0                 Valsartan  ...   8.022969\n",
              "1                Guanfacine  ...   7.858458\n",
              "2                    Lybrel  ...   6.341969\n",
              "3  Buprenorphine / naloxone  ...   6.590176\n",
              "4                    Cialis  ...   6.144782\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjnmvY1nHpFq",
        "colab_type": "code",
        "outputId": "173b2e8c-ad42-4ce8-a55c-8d42f29729f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# remove space\n",
        "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
        "def remove_space(text):\n",
        "    \"\"\"\n",
        "    remove extra spaces and ending space if any\n",
        "    \"\"\"\n",
        "    for space in spaces:\n",
        "        text = text.replace(space, ' ')\n",
        "    text = text.strip()\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# replace strange punctuations and raplace diacritics\n",
        "from unicodedata import category, name, normalize\n",
        "\n",
        "def remove_diacritics(s):\n",
        "    return ''.join(c for c in normalize('NFKD', s.replace('√∏', 'o').replace('√ò', 'O').replace('‚Åª', '-').replace('‚Çã', '-'))\n",
        "                  if category(c) != 'Mn')\n",
        "\n",
        "special_punc_mappings = {\"‚Äî\": \"-\", \"‚Äì\": \"-\", \"_\": \"-\", '‚Äù': '\"', \"‚Ä≥\": '\"', '‚Äú': '\"', '‚Ä¢': '.', '‚àí': '-',\n",
        "                         \"‚Äô\": \"'\", \"‚Äò\": \"'\", \"¬¥\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ÿå':'','‚Äû':'',\n",
        "                         '‚Ä¶': ' ... ', '\\ufeff': ''}\n",
        "def clean_special_punctuations(text):\n",
        "    for punc in special_punc_mappings:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, special_punc_mappings[punc])\n",
        "    text = remove_diacritics(text)\n",
        "    return text\n",
        "\n",
        "# clean numbers\n",
        "def clean_number(text):\n",
        "    if bool(re.search(r'\\d', text)):\n",
        "        text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text) # digits followed by a single alphabet...\n",
        "        text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text) #1st, 2nd, 3rd, 4th...\n",
        "        text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
        "    return text\n",
        "\n",
        "import string\n",
        "regular_punct = list(string.punctuation)\n",
        "extra_punct = [\n",
        "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
        "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '‚Ä¢',  '~', '@', '¬£',\n",
        "    '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',\n",
        "    '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù',\n",
        "    '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ',\n",
        "    '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº',\n",
        "    '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤',\n",
        "    '√®', '¬∏', '¬æ', '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª',\n",
        "    'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò',\n",
        "    '¬π', '‚â§', '‚Ä°', '‚àö', '¬´', '¬ª', '¬¥', '¬∫', '¬æ', '¬°', '¬ß', '¬£', '‚Ç§',\n",
        "    ':)', ': )', ':-)', '(:', '( :', '(-:', ':\\')',\n",
        "    ':D', ': D', ':-D', 'xD', 'x-D', 'XD', 'X-D',\n",
        "    '<3', ':*',\n",
        "    ';-)', ';)', ';-D', ';D', '(;',  '(-;',\n",
        "    ':-(', ': (', ':(', '\\'):', ')-:',\n",
        "    '-- :','(', ':\\'(', ':\"(\\'',]\n",
        "\n",
        "def handle_emojis(text): #Speed can be improved via a simple if check :)\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
        "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', text)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', text)\n",
        "    # Love -- <3, :*\n",
        "    text = re.sub(r'(<3|:\\*)', ' EMO_POS ', text)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
        "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', text)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:\n",
        "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', text)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', text)\n",
        "    return text\n",
        "\n",
        "def stop(text):\n",
        "    \n",
        "    from nltk.corpus import stopwords\n",
        "    \n",
        "    text = \" \".join([w.lower() for w in text.split()])\n",
        "    stop_words = stopwords.words('english')\n",
        "    \n",
        "    words = [w for w in text.split() if not w in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "all_punct = list(set(regular_punct + extra_punct))\n",
        "# do not spacing - and .\n",
        "all_punct.remove('-')\n",
        "all_punct.remove('.')\n",
        "\n",
        "# clean repeated letters\n",
        "def clean_repeat_words(text):\n",
        "    \n",
        "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
        "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
        "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
        "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
        "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
        "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
        "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
        "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
        "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
        "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
        "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
        "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
        "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
        "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
        "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
        "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
        "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
        "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
        "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
        "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
        "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
        "    text = re.sub(r\"plzz+\", \"please\", text)\n",
        "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
        "    text = re.sub(r\"(-+|\\.+)\", \" \", text) #new haha #this adds a space token so we need to remove xtra spaces\n",
        "    return text\n",
        "\n",
        "def spacing_punctuation(text):\n",
        "    \"\"\"\n",
        "    add space before and after punctuation and symbols\n",
        "    \"\"\"\n",
        "    for punc in all_punct:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, f' {punc} ')\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    preprocess text main steps\n",
        "    \"\"\"\n",
        "    text = remove_space(text)\n",
        "    text = clean_special_punctuations(text)\n",
        "    text = handle_emojis(text)\n",
        "    text = clean_number(text)\n",
        "    text = spacing_punctuation(text)\n",
        "    text = clean_repeat_words(text)\n",
        "    text = remove_space(text)\n",
        "    #text = stop(text)# if changing this, then chnage the dims \n",
        "    #(not to be done yet as its effecting the embeddings..,we might be\n",
        "    #loosing words)...\n",
        "    return text\n",
        "\n",
        "mispell_dict = {'üòâ':'wink','üòÇ':'joy','üòÄ':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
        "\n",
        "def correct_spelling(x, dic):\n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x\n",
        "\n",
        "def correct_contraction(x, dic):\n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(2411)\n",
        "SEED = 42\n",
        "num_partitions = 10  # number of partitions to split dataframe\n",
        "num_cores = psutil.cpu_count()  # number of cores on your machine\n",
        "\n",
        "print('number of cores:', num_cores)\n",
        "\n",
        "def df_parallelize_run(df, func):\n",
        "    \n",
        "    df_split = np.array_split(df, num_partitions)\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of cores: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5RDgIX-Klf9",
        "colab_type": "code",
        "outputId": "7d71db80-3dfa-4cdd-d4a4-d81bc42c1fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def text_clean_wrapper(df):\n",
        "    \n",
        "    df[\"review_by_patient\"] = df[\"review_by_patient\"].astype('str').transform(preprocess)\n",
        "    df['review_by_patient'] = df['review_by_patient'].transform(lambda x: correct_spelling(x, mispell_dict))\n",
        "    df['review_by_patient'] = df['review_by_patient'].transform(lambda x: correct_contraction(x, contraction_mapping))\n",
        "    \n",
        "    return df\n",
        "\n",
        "#fast!\n",
        "train = df_parallelize_run(df, text_clean_wrapper)\n",
        "test  = df_parallelize_run(df1, text_clean_wrapper)\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del mispell_dict, all_punct, special_punc_mappings, regular_punct, extra_punct\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1amVEVmFMkYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_seq(train, test):\n",
        "    '''\n",
        "        credits go to: https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution/ \n",
        "    '''\n",
        "    \n",
        "    tokenizer = Tokenizer() \n",
        "    tokenizer.fit_on_texts(list(train['review_by_patient']) + list(test['review_by_patient']))\n",
        "    word_index = tokenizer.word_index\n",
        "    X_train = tokenizer.texts_to_sequences(list(train['review_by_patient']))\n",
        "    X_test = tokenizer.texts_to_sequences(list(test['review_by_patient']))\n",
        "    X_train = pad_sequences(X_train,maxlen = 180)\n",
        "    X_test = pad_sequences(X_test, maxlen = 180)\n",
        "    \n",
        "    return X_train, X_test, word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7XQr2GXa_7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, word_index = run_seq(train,test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6qg9qevvI7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMB_PATHS = [\n",
        "    'crawl-300d-2M.vec',\n",
        "    'glove.840B.300d.txt'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRMblwlYk36V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path) as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "\n",
        "def build_embedding_matrix(word_index, path):\n",
        "    '''\n",
        "     credits to: https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
        "    '''\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "        except:\n",
        "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
        "            \n",
        "    del embedding_index\n",
        "    gc.collect()\n",
        "    return embedding_matrix\n",
        "\n",
        "def build_embeddings(word_index):\n",
        "    '''\n",
        "     credits to: https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
        "     credits go to: https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution/ \n",
        "    '''\n",
        "    embedding_matrix = np.concatenate(\n",
        "        [build_embedding_matrix(word_index, f) for f in EMB_PATHS], axis=-1) \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bF7TtYLb7GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = build_embeddings(word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYndVTUm6LjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.callbacks import *\n",
        "from keras.optimizers import *\n",
        "def rmse(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTaS86OqEDq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "\n",
        "\n",
        "        \n",
        "def coeff_determination(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OkpVVjbOV84M",
        "colab": {}
      },
      "source": [
        "df['target'] = np.around(df.base_score.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62OvIKVo0Wg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(lr=0.0, lr_d=0.0, spatial_dr=0.0,  dense_units=128, dr=0.1):\n",
        "    \n",
        "    from keras.layers import LSTM, Bidirectional, Dropout\n",
        "    \n",
        "    file_path = \"best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=3, min_lr=0.001)\n",
        "    inp = Input(shape=(180,))\n",
        "\n",
        "    x = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = SpatialDropout1D(spatial_dr)(x)\n",
        "    \n",
        "    x1 = Bidirectional(CuDNNLSTM(180, return_sequences=True))(x)\n",
        "    x2 = Bidirectional(CuDNNGRU(180, return_sequences=True))(x1)\n",
        "    att1 = Attention(180)(x1)\n",
        "    att2 = Attention(180)(x2)\n",
        "    x3 = Average()([att1,att2])\n",
        "    max_pool1 = GlobalAveragePooling1D()(x1)\n",
        "    max_pool2 = GlobalAveragePooling1D()(x2)\n",
        "    conc = Concatenate()([x3,max_pool1, max_pool2])\n",
        "    x = Dense(200,activation = 'elu')(conc)\n",
        "    x = Dense(50,activation = 'elu')(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=predictions)\n",
        "    model.compile(loss = rmse, optimizer = Adam(lr = lr, decay = lr_d))\n",
        "\n",
        "    history = model.fit(X_train,df.base_score.values, batch_size = 512, epochs = 100, validation_split=0.2, \n",
        "                        verbose = 1, callbacks = [check_point,reduce_lr])\n",
        "    \n",
        "    #model = load_model(file_path)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "116a66f9-350f-4303-de4d-a4314a4c4adc",
        "id": "FN2AM6meV-W9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model(lr = 5e-2, lr_d = 0.001, spatial_dr = 0.23, dr=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25732 samples, validate on 6433 samples\n",
            "Epoch 1/100\n",
            "25732/25732 [==============================] - 28s 1ms/step - loss: 5.6769 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.65728, saving model to best_model.hdf5\n",
            "Epoch 2/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 5.65728\n",
            "Epoch 3/100\n",
            "25732/25732 [==============================] - 25s 987us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 5.65728\n",
            "Epoch 4/100\n",
            "25732/25732 [==============================] - 25s 989us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 5.65728\n",
            "Epoch 5/100\n",
            "25732/25732 [==============================] - 25s 988us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 5.65728\n",
            "Epoch 6/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 5.65728\n",
            "Epoch 7/100\n",
            "25732/25732 [==============================] - 25s 989us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 5.65728\n",
            "Epoch 8/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 5.65728\n",
            "Epoch 9/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 5.65728\n",
            "Epoch 10/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 5.65728\n",
            "Epoch 11/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 5.65728\n",
            "Epoch 12/100\n",
            "25732/25732 [==============================] - 25s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 5.65728\n",
            "Epoch 13/100\n",
            "25732/25732 [==============================] - 26s 993us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 5.65728\n",
            "Epoch 14/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 5.65728\n",
            "Epoch 15/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 5.65728\n",
            "Epoch 16/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 5.65728\n",
            "Epoch 17/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.65728\n",
            "Epoch 18/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.65728\n",
            "Epoch 19/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 5.65728\n",
            "Epoch 20/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.65728\n",
            "Epoch 21/100\n",
            "25732/25732 [==============================] - 25s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 5.65728\n",
            "Epoch 22/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 5.65728\n",
            "Epoch 23/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 5.65728\n",
            "Epoch 24/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 5.65728\n",
            "Epoch 25/100\n",
            "25732/25732 [==============================] - 25s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 5.65728\n",
            "Epoch 26/100\n",
            "25732/25732 [==============================] - 26s 993us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 5.65728\n",
            "Epoch 27/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 5.65728\n",
            "Epoch 28/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 5.65728\n",
            "Epoch 29/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 5.65728\n",
            "Epoch 30/100\n",
            "25732/25732 [==============================] - 25s 990us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 5.65728\n",
            "Epoch 31/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 5.65728\n",
            "Epoch 32/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 5.65728\n",
            "Epoch 33/100\n",
            "25732/25732 [==============================] - 25s 989us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 5.65728\n",
            "Epoch 34/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 5.65728\n",
            "Epoch 35/100\n",
            "25732/25732 [==============================] - 25s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 5.65728\n",
            "Epoch 36/100\n",
            "25732/25732 [==============================] - 25s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 5.65728\n",
            "Epoch 37/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 5.65728\n",
            "Epoch 38/100\n",
            "25732/25732 [==============================] - 25s 989us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 5.65728\n",
            "Epoch 39/100\n",
            "25732/25732 [==============================] - 26s 992us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 5.65728\n",
            "Epoch 40/100\n",
            "25732/25732 [==============================] - 26s 991us/step - loss: 5.6666 - val_loss: 5.6573\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 5.65728\n",
            "Epoch 41/100\n",
            "20992/25732 [=======================>......] - ETA: 4s - loss: 5.6645"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "07-Bv1dUV95n",
        "colab": {}
      },
      "source": [
        "model.summay()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts3CLOK8PDuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}